---
title: "DS 6372 Project 1"
output: html_document
---

# Data Prep

Load the necessary packages
```{r message=FALSE}
library(tidyverse)
library(janitor)
library(naniar)
library(polycor)
library(cowplot)
library(corrplot)
library(GGally)
library(leaps)
library(olsrr)
library(caret)
```
Read in the data, clean variable names
```{r}
cars <- clean_names(read_csv('https://raw.githubusercontent.com/hmlam1/MSDS-Projects/main/Applied%20Stats%20Project%201/Vehicle_MSRP.csv', show_col_types=F))
```
Raw data prior to cleaning: 16 variables, 11914 observations
```{r}
dim(cars)
```
Check for missing values for each variable: engine_hp (69), engine_cylinders (30), number_of_doors (6), engine_fuel_type (3)
```{r}
miss_var_summary(cars)
```
Replace number_of_doors with actual value, replace highway_mpg with real value
```{r}
cars <- cars %>% mutate(number_of_doors = ifelse(make == 'Tesla' & model == 'Model S' & year == 2016,4, ifelse(make == 'Ferrari' & model == 'FF' & year == 2013,2,number_of_doors)), highway_mpg = ifelse(highway_mpg == 354, 34, highway_mpg))
```
Replace missing value of engine_cylinders to 0 for electric cars, remove 20 missing values for rotary engine
```{r}
cars <- cars %>% mutate(engine_cylinders = ifelse(engine_fuel_type == 'electric', 0, engine_cylinders)) %>% filter(!is.na(engine_cylinders))
```
Replace missing value of Suzuki Verona (engine_cylinders, engine_fuel)
```{r}
cars <- cars %>% mutate(engine_cylinders = ifelse(model == 'Verona', 6, engine_cylinders), engine_fuel_type = ifelse(model  == 'Verona', 'regular unleaded', engine_fuel_type))
```
Replace engine_hp with value, remove the last 3 obs
```{r}
cars <- cars %>% mutate(engine_hp = ifelse(model == '500e', 111, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Continental' & make == 'Lincoln', 350, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Escape' & make == 'Ford', 206, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Fit EV' & make == 'Honda', 120, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Focus' & make == 'Ford', 143, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Freestar' & make == 'Ford', 195, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Impala' & make == 'Chevrolet', 196, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Leaf' & make == 'Nissan', 107, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'MKZ' & make == 'Lincoln', 188, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Model S' & make == 'Tesla', 400, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'RAV4 EV' & make == 'Toyota', 154, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Soul EV' & make == 'Kia', 109, engine_hp))
cars <- cars %>% filter(!is.na(engine_hp))
```
Confirm that there are zero NA values
```{r}
sum(as.numeric(is.na.data.frame(cars)))
```
Filter observations where msrp is between 2001-150000, year after 2000
```{r}
cars <- cars %>% filter(msrp != 2000 & msrp < 150000 & year >= 2001)
# filter further cars <- cars %>% filter(!(year<=2010 & msrp>= 100000))
```
set relevant variables as factors
```{r}
factor_vars <- c("make", "model", "engine_fuel_type", "engine_cylinders", "transmission_type", "driven_wheels", "number_of_doors", "market_category", "vehicle_size", "vehicle_style")
cars[factor_vars] <- lapply(cars[factor_vars], factor)
```
Study the data: 16 variables, 9824 observations
```{r}
str(cars)
summary(cars)
```
# EDA

## Numerical variables

Correlation pattern. Engine_hp with highest corr (.797) and popularity with lowest (0.0222)
```{r}
i1 <- sapply(cars, is.numeric)
y1 <- 'msrp'
x1 <- setdiff(names(cars)[i1], y1)
cor(cars[x1], cars[[y1]])
```
visual representation of correlation matrix
```{r}
num_corr <- cor(cars[x1], cars[[y1]])
cnames <- c('year', 'engine_hp', 'highway_mpg', 'city_mpg', 'popularity')
num_corr <- cbind(num_corr, cnames) %>% as.data.frame()
num_corr %>% ggplot(aes(x = cnames, y = V1, color = cnames))+geom_point()+theme_classic()+ggtitle("MSRP correlation with numeric variables")+labs(x = "Numeric Variables", y = 'Correlation')+theme(legend.position='none')
```
Plot numerical variables
```{r}
# msrp. log transform
p1 <- cars %>% ggplot(aes(x=msrp)) + geom_histogram(bins=20)
# logged: cars %>% ggplot(aes(x=log(msrp))) + geom_histogram(bins=20)

# msrp vs. year. 
p2 <- cars %>% ggplot(aes(x=year, y=msrp)) + geom_point()

# msrp vs. popularity
## Popularity is a weird variable. Score is set by brand rather than model. And Ford has the highest pop score by far, but certainly not the highest msrp, so I suspect this var won't be used for predicting msrp. can be set to categorical if necessary
p3 <- cars %>% ggplot(aes(x=popularity, y=msrp)) + geom_point()

# msrp vs. highway_mpg
p4 <- cars %>% ggplot(aes(x=highway_mpg, y=msrp)) + geom_point() + geom_smooth(method='lm')

# msrp vs. city_mpg
p5 <- cars %>% ggplot(aes(x=city_mpg, y=msrp)) + geom_point() + geom_smooth(method='lm')

# msrp vs. engine_hp
p6 <- cars %>% ggplot(aes(x=engine_hp, y=msrp)) + geom_point() + geom_smooth(method='lm')

# PLOT
plot_grid(p1, p2, p3, p4, p5, p6)
```

## Categorical variables

Correlation of categorical variables 
```{r warning=FALSE}
# caution: code takes a hot min to run
hetcor(cars$msrp, cars$engine_fuel_type, cars$model, cars$make, cars$engine_cylinders, cars$transmission_type, cars$driven_wheels, cars$number_of_doors, cars$market_category, cars$vehicle_size, cars$vehicle_style)

### it appears market_category has the highest correlation with msrp (0.7203), followed by engine_cylinders with 0.6. Of insignificant correlation are model (.012), driven wheels(0.055) and number of doors(.1179)
```
msrp vs. make, vehicle_style, and market_category (did not plot msrp vs. model bc unreadable at 65 levels)
```{r}
# msrp vs. make
p7 <- cars %>% ggplot(aes(x=make, y=msrp)) + geom_boxplot()

# msrp vs. vehicle_style
vs_levels <- cars %>% select(msrp, vehicle_style) %>%
  group_by(vehicle_style) %>% summarize(median(msrp)) %>% arrange(desc(`median(msrp)`))

cars$vehicle_style <- factor(cars$vehicle_style, ordered=TRUE, levels=vs_levels$vehicle_style)
# boxplot
p8 <- cars %>% ggplot(aes(y = msrp, fill = vehicle_style))+geom_boxplot()

# msrp vs market_category
cars %>% select(msrp, market_category) %>%
  ggplot(aes(x = msrp, fill=market_category))+
  geom_histogram()+theme(legend.position='none')
# boxplot
p9 <- cars %>% ggplot(aes(x=market_category, y=msrp)) + geom_boxplot()

# PLOT ALL
plot_grid(p7, p8, p9, nrow = 3)
```

```{r}
mc_levels <- cars %>% select(msrp, market_category) %>%
  group_by(market_category) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$market_category <- factor(cars$market_category, ordered = TRUE, levels = mc_levels$market_category)

summary(cars$market_category)
```

Plotting rest of the categorical variables together
```{r}
# msrp vs. engine_cylinders
# ordered levels
ec_levels <- cars %>% select(msrp, engine_cylinders) %>%
  group_by(engine_cylinders) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$engine_cylinders <- factor(cars$engine_cylinders, ordered=TRUE, levels=ec_levels$engine_cylinders)

p10 <- cars %>% ggplot(aes(x=engine_cylinders, y=msrp)) + geom_boxplot()

## msrp vs. engine_fuel_type
p11 <- cars %>% ggplot(aes(x=engine_fuel_type, y=msrp)) + geom_boxplot()

## msrp vs. driven_wheels
p12 <- cars %>% ggplot(aes(x=driven_wheels, y=msrp)) + geom_boxplot()

# msrp vs. transmission_type
tt_levels <- cars %>% select(msrp, transmission_type) %>%
  group_by(transmission_type) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$transmission_type <- factor(cars$transmission_type, ordered=TRUE, levels=tt_levels$transmission_type)

p13 <- cars %>% ggplot(aes(y = msrp, fill = transmission_type))+geom_boxplot()

# msrp vs. vehicle_size
# ordering vehicle_size
vsi_levels <- cars %>% select(msrp, vehicle_size) %>%
  group_by(vehicle_size) %>% summarize(median(msrp)) %>% arrange((`median(msrp)`))

cars$vehicle_size <- factor(cars$vehicle_size, ordered=TRUE, levels=vsi_levels$vehicle_size)

p14 <- cars %>% ggplot(aes(x=vehicle_size, y=msrp)) + geom_boxplot()

## msrp vs. number_of_doors
p15 <- cars %>% ggplot(aes(x=number_of_doors, y=msrp)) + geom_boxplot()

## PLOT ALL
plot_grid(p10, p11, p12, p13, p14, p15)
```

# Simple Model

Split data
```{r}
set.seed(7)
library(splitTools) # data partition

index <- partition(cars$logmsrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]
```
log(msrp). Correlation matrix with *numerical* variables : two diff plots. Which do yall prefer?
```{r}
cars <- cars %>% mutate(logmsrp = log(msrp))
cor.numvars <- cars %>% select(logmsrp, popularity, year, engine_hp, highway_mpg, city_mpg) %>% cor()

# correlogram / high correlation = blue color and larger size / library(corrplot) ; 
corrplot(cor.numvars, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

# correlation chart / library(GGally)
ggpairs(cars, columns = colnames(cor.numvars))

# popularity has low correlation but is required for the assignment. maybe run without it later
# include all num var in model. all corr > .2
# highway, city mpg have slightly higher corr with logmsrp when logged; keep unlogged bc negligible
```
log(msrp). Correlation with *categorical* variables : two diff plots
```{r warning=FALSE}
hetcor(cars$logmsrp, cars$engine_fuel_type, cars$engine_cylinders, cars$market_category, cars$vehicle_size, cars$make, cars$model, cars$transmission_type, cars$driven_wheels, cars$number_of_doors, cars$vehicle_style)
# caution: takes a few mins to run. here is output to save you time in desc order
# engine_cylinders : .5578 # keep in model
# engine_fuel_type : -.4082 # keep in model
# vehicle_size : .2766 # keep in model
# transmission_type : -.2544 # keep in model
# make : -.2271 # maybe keep in model
# market_category : -.1893
# driven_wheels : -.07438
# number_of_doors : -.06646
# vehicle_style : .04083
# model : -.01161

# eda in order of highest correlation with logmsrp; check if factors are ordered
# did not include make, model, market_category bc insanely high # of levels

pairs <- ggpairs(data=cars, columns = c('logmsrp', 'engine_cylinders', 'engine_fuel_type', 'vehicle_size', 'transmission_type', 'driven_wheels', 'number_of_doors', 'vehicle_style'), cardinality_threshold = 16)

plots <- lapply(1:pairs$ncol, function(j) getPlot(pairs, i = 1, j = j))
ggmatrix(
plots,
nrow = 1,
ncol = pairs$ncol,
xAxisLabels = pairs$xAxisLabels,
yAxisLabels = pairs$yAxisLabels
)
```
Custom variable selection
```{r}
# chose all numerical vars (corr > 20% w logmsrp) and top 4 cat. vars (corr > 25% w logmsrp)

mod1 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, data=cars)
ols_vif_tol(mod1) # high vif for engine_cylinders, transmission_type, city_mpg so remove them for mod2
mod2 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars) 
ols_vif_tol(mod2) # vif still high, remove engine_fuel_type for mod3
mod3 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars)
ols_vif_tol(mod3) # vifs look great
ols_regress(mod3) # view model summary, estimates

# now to test/train data
set.seed(7)
library(splitTools) # data partition

index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]

simple.train <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=train)
ols_regress(simple.train)
# train metrics: adjr2=.779 rmse=212 ase=.045

# validation
#Really handy predict function
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

# validation
simple.train <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=train)
# predict test
simple.test <-predict.regsubsets(object=simple.train, newdata=test)

# predict validate
simple.valid <-predict.regsubsets(object=simple.train, newdata=valid)

metrics_galore <-function(actual, predicted){
    result<-c(mean((actual- predicted)^2),
              caret::MAE(actual, predicted),
              rmse = caret::RMSE(actual, predicted),
              1-(  (1-(caret::R2(actual, predicted)))*(nrow(train))/(nrow(train)- ncol(train)-1)))
    names(result)<-c("ASE","MAE","RMSE","Adjusted R2")
    return(result)
}

metrics_galore(test$logmsrp, simple.test)
metrics_galore(valid$logmsrp, simple.valid)

```
Just for kicks: Lasso with variables from mod1 to compare with my custom model
```{r}
library(glmnet)
set.seed(7)
index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]

x=model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, train)
y=train$logmsrp
xtest<-model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, test)
ytest<-test$logmsrp
lasso.mod <- glmnet(x,y,alpha=1)
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out) # 21 predictors chosen
bestlambda<-cv.out$lambda.min
bestlambda
lasso.pred <- predict(lasso.mod, s=bestlambda, newx=xtest)
testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO # 0.0397921
coef(lasso.mod, s=bestlambda) # view predictors
metrics_galore(ytest, lasso.pred)

# test ASE is smaller than my custom model by .003; negligible difference but more variables; keep previous model instead
```
Fit model, assumptions check
```{r}
mod3 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars)
ols_step_forward_aic(mod3) # view aic, ss, rss, r2, adjr2 (last row)
plot(ols_step_all_possible(mod3), print_plot = F) # view cp mallows, aic, sbic, sbc
ols_plot_diagnostics(mod3, print_plot = FALSE)
# residual histogram, cook's d chart looks great
# qq plot pretty good. some veering off at both extreme ends
# veryyyy few obs with outlier & leverage (expected for sample size this large)
# residual vs predicted looks good but some veering off at upper end
```

# Complex Model
```{r}
# Bar graph of vehicle size, same information as EDA boxplot
train %>% group_by(vehicle_size) %>% summarize(avg_msrp = mean(msrp)) %>% 
  ggplot(aes(x=vehicle_size, y=avg_msrp)) + geom_bar(stat = 'identity')

# Depicting indication of engine_hp's relationship to msrp depending on vehicle_size (2-way interaction)
train %>% ggplot(aes(x = engine_hp, y = logmsrp, color = vehicle_size)) + geom_point()

# Potential indication that city_mpg depends on the year
train %>% ggplot(aes(x = city_mpg, y = logmsrp, color = as.factor(year))) + geom_point()

# Modifying train dataset to reduce engine_fuel_type categories (high VIF) and market_category(luxury brands)
train2 <- train %>% mutate(premiumfuel = ifelse(grepl('premium',engine_fuel_type),'premium','not premium'),
                           luxury = ifelse(grepl('Luxury',market_category),'luxury','not luxury'))

# Creating same variables for test set in order to run the model using derived variables
test2 <- test %>% mutate(premiumfuel = ifelse(grepl('premium',engine_fuel_type),'premium','not premium'),
                         luxury = if_else(grepl('Luxury',market_category),'luxury','not luxury'))

# Creating same variables for original set in order to check residual diagnostics
cars2 <- cars %>% mutate(premiumfuel = ifelse(grepl('premium',engine_fuel_type),'premium','not premium'),
                         luxury = if_else(grepl('Luxury',market_category),'luxury','not luxury'))

# Creating same variables for validation set in order to run predictions later
valid2 <- valid %>% mutate(premiumfuel = ifelse(grepl('premium',engine_fuel_type),'premium','not premium'),
                         luxury = if_else(grepl('Luxury',market_category),'luxury','not luxury'))

# Testing out multiple models, all models include luxury variable
# City MPG x Year 
complexmod1 <- lm(logmsrp~popularity+engine_hp+vehicle_size+year*city_mpg+highway_mpg+engine_cylinders+engine_fuel_type+transmission_type+luxury, train2)

# Engine HP x Vehicle Size
complexmod2 <- lm(logmsrp~popularity+engine_hp*vehicle_size+year+city_mpg+highway_mpg+engine_cylinders+engine_fuel_type+transmission_type+luxury, train2)

# Both interactions
complexmod3 <- lm(logmsrp~popularity+engine_hp*vehicle_size+year*city_mpg+highway_mpg+engine_cylinders+engine_fuel_type+transmission_type+luxury, train2)

summary(complexmod1)
summary(complexmod2)
summary(complexmod3)

# Predictions based on second of three models (engine_hp x vehicle_size, +luxury)
complex.pred <- predict(complexmod2, newdata = test2)

# ASE as low as 0.02908
testMSE_complex <- mean((ytest-complex.pred)^2)

# For residual diagnostics
checkcomplexmod2 <- lm(logmsrp~popularity+engine_hp*vehicle_size+year+city_mpg+highway_mpg+engine_cylinders+engine_fuel_type+transmission_type+luxury, cars2)

# Diagnostics look fine
ols_plot_diagnostics(checkcomplexmod2, print_plot = FALSE)

# Validation
complex.train <- complexmod2
# Predict test
complex.test <- predict(complex.train, newdata=test2)
# Predict validate
complex.valid <- predict(complex.train, newdata=valid2)

metrics_galore(test2$logmsrp, complex.test)
metrics_galore(valid2$logmsrp, complex.valid)
```

# Nonparametric Model
```{r}
# decision tree but each leaf represents a numberic value.
# splits are determined by trying a certain amount of thresholds and calculating Sum Squared Residuals
# for predictions, the average value of the bucket/leaf that an observation falls into is what is used as the predicted value. 
# a linear relationship between independent and dependent variables is no longer needed, and "interactions" do not need to be specified. 
# risk of overfitting by including too many variables that do not add value (bias/variance trade off) 
# risk of overfitting by including too many "leaves", too high of a tree depth
# this is mitigated by limiting minimum number of observations needed for a given bucket, ensuring next split improves CP metric by a certain amount, or limiting overall tree depth. 
#function for calculating ASE
ASE <- function(actual, predicted) {(mean ((actual - predicted)^2))}
set.seed(7)
library(splitTools) # data partition
index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]
#first model using min bucket size of 100 to prevent overfitting and high correlation variables only
tree1 <- rpart(log(msrp) ~ highway_mpg + year + engine_hp + market_category + engine_cylinders, data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))
treep1 <- exp(predict(tree1, test))
rpart.plot(tree1, type = 1, digits = 3, fallen.leaves = TRUE)
ASE(test$msrp, treep1)
sqrt(ASE(test$msrp, treep1))
#model two like first but using unlogged msrp
tree2 <- rpart(msrp ~ highway_mpg + year + engine_hp + market_category + engine_cylinders, data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))
treep2 <- predict(tree2, test)
rpart.plot(tree2, type = 1, digits = 3, fallen.leaves = TRUE)
ASE(test$msrp, treep2)
sqrt(ASE(test$msrp, treep2))
#model 3 with all variables except model and unlogged msrp
tree3 <- rpart(msrp ~ engine_hp + market_category + make + engine_cylinders + vehicle_size + engine_fuel_type + city_mpg + vehicle_style + highway_mpg + popularity, data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))
treep3 <- predict(tree3, test)
rpart.plot(tree3, type = 1, digits = 3, fallen.leaves = TRUE)
ASE(test$msrp, treep3)
sqrt(ASE(test$msrp, treep3))

#model 4 with caret all variables except model - performs worse but less risk of overfitting
trctrl <- trainControl(method = 'repeatedcv', repeats = 3, number = 10)
tree4 <- train(msrp ~ engine_hp + market_category + make + engine_cylinders + vehicle_size + engine_fuel_type + city_mpg + vehicle_style + highway_mpg + popularity, data = train, method = "rpart2")
treep4 <- predict(tree4, test)
ASE(test$msrp, treep4)
sqrt(ASE(test$msrp, treep4))
rpart.plot(tree4$finalModel, type = 1, digits = 3, fallen.leaves = TRUE)

#graphing
test2 <- test
test2$predicted1 <- treep1
test2$predicted2 <- treep2
test2$predicted3 <- treep3
test2$predicted4 <- treep4
p1 <- test2 %>% ggplot(aes(x = predicted1, y = msrp)) + geom_point() + geom_smooth(method = 'lm')
p2 <- test2 %>% ggplot(aes(x = predicted2, y = msrp)) + geom_point() + geom_smooth(method = 'lm')
p3 <- test2 %>% ggplot(aes(x = predicted3, y = msrp)) + geom_point() + geom_smooth(method = 'lm') + ggtitle("Regression Tree Predictions vs Actual MSRP") + xlab("Predicted MSRP")
p4 <- test2 %>% ggplot(aes(x = predicted4, y = msrp)) + geom_point() + geom_smooth(method = 'lm') 

library(cowplot)
plot_grid(p1, p2, p3, p4)
# RMSE and AIC for each model
library(regclass)
summarize_tree(tree1)
summarize_tree(tree2)
summarize_tree(tree3)
summarize_tree(tree4$finalModel)

```
# Compare all the models
```{r}
metrics_galore <-function(actual, predicted){
    result<-c(mean((actual- predicted)^2),
              caret::MAE(actual, predicted),
              rmse = caret::RMSE(actual, predicted),
              1-(  (1-(caret::R2(actual, predicted)))*(nrow(train))/(nrow(train)- ncol(train)-1)))
    names(result)<-c("ASE","MAE","RMSE","Adjusted R2")
    return(result)
}
#### Predict TEST
# Simple model
simple.train <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=train)
simple.test <-predict.regsubsets(object=simple.train, newdata=test)
metrics_galore(test$logmsrp, simple.test)

# Complex model

# Nonparametric model
np.train <- tree
np.test <- predict(tree3, test)
format(metrics_galore(test$msrp, np.test), scientific = FALSE)

#### Predict VALIDATION 
# Simple model
simple.valid <-predict.regsubsets(object=simple.train, newdata=valid)
metrics_galore(valid$logmsrp, simple.valid)
# Complex model

# Nonparametric model
np.valid <- predict(tree3, valid)
format(metrics_galore(valid$msrp, np.valid), scientific = FALSE)
```

