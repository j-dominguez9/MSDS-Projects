---
title: "DS 6372 Project 1"
output: html_document
---

# Data Prep

Load the necessary packages
```{r message=FALSE}
library(tidyverse)
library(janitor)
library(naniar)
library(polycor)
library(cowplot)
library(corrplot)
library(GGally)
library(leaps)
library(olsrr)
library(caret)
```
Read in the data, clean variable names
```{r}
cars <- clean_names(read_csv('https://raw.githubusercontent.com/hmlam1/MSDS-Projects/main/Applied%20Stats%20Project%201/Vehicle_MSRP.csv', show_col_types=F))
```
Raw data prior to cleaning: 16 variables, 11914 observations
```{r}
dim(cars)
```
Check for missing values for each variable: engine_hp (69), engine_cylinders (30), number_of_doors (6), engine_fuel_type (3)
```{r}
miss_var_summary(cars)
```
Replace number_of_doors with actual value, replace highway_mpg with real value
```{r}
cars <- cars %>% mutate(number_of_doors = ifelse(make == 'Tesla' & model == 'Model S' & year == 2016,4, ifelse(make == 'Ferrari' & model == 'FF' & year == 2013,2,number_of_doors)), highway_mpg = ifelse(highway_mpg == 354, 34, highway_mpg))
```
Replace missing value of engine_cylinders to 0 for electric cars, remove 20 missing values for rotary engine
```{r}
cars <- cars %>% mutate(engine_cylinders = ifelse(engine_fuel_type == 'electric', 0, engine_cylinders)) %>% filter(!is.na(engine_cylinders))
```
Replace missing value of Suzuki Verona (engine_cylinders, engine_fuel)
```{r}
cars <- cars %>% mutate(engine_cylinders = ifelse(model == 'Verona', 6, engine_cylinders), engine_fuel_type = ifelse(model  == 'Verona', 'regular unleaded', engine_fuel_type))
```
Replace engine_hp with value, remove the last 3 obs
```{r}
cars <- cars %>% mutate(engine_hp = ifelse(model == '500e', 111, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Continental' & make == 'Lincoln', 350, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Escape' & make == 'Ford', 206, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Fit EV' & make == 'Honda', 120, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Focus' & make == 'Ford', 143, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Freestar' & make == 'Ford', 195, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Impala' & make == 'Chevrolet', 196, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Leaf' & make == 'Nissan', 107, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'MKZ' & make == 'Lincoln', 188, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Model S' & make == 'Tesla', 400, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'RAV4 EV' & make == 'Toyota', 154, engine_hp))
cars <- cars %>% mutate(engine_hp = ifelse(model == 'Soul EV' & make == 'Kia', 109, engine_hp))
cars <- cars %>% filter(!is.na(engine_hp))
```
Confirm that there are zero NA values
```{r}
sum(as.numeric(is.na.data.frame(cars)))
```
Filter observations where msrp is between 2001-150000, year after 2000
```{r}
cars <- cars %>% filter(msrp != 2000 & msrp < 150000 & year >= 2001)
# filter further cars <- cars %>% filter(!(year<=2010 & msrp>= 100000))
```
set relevant variables as factors
```{r}
factor_vars <- c("make", "model", "engine_fuel_type", "engine_cylinders", "transmission_type", "driven_wheels", "number_of_doors", "market_category", "vehicle_size", "vehicle_style")
cars[factor_vars] <- lapply(cars[factor_vars], factor)
```
Study the data: 16 variables, 9824 observations
```{r}
str(cars)
summary(cars)
```
# EDA

## Numerical variables

Correlation pattern. Engine_hp with highest corr (.797) and popularity with lowest (0.0222)
```{r}
i1 <- sapply(cars, is.numeric)
y1 <- 'msrp'
x1 <- setdiff(names(cars)[i1], y1)
cor(cars[x1], cars[[y1]])
```
visual representation of correlation matrix
```{r}
num_corr <- cor(cars[x1], cars[[y1]])
cnames <- c('year', 'engine_hp', 'highway_mpg', 'city_mpg', 'popularity')
num_corr <- cbind(num_corr, cnames) %>% as.data.frame()
num_corr %>% ggplot(aes(x = cnames, y = V1, color = cnames))+geom_point()+theme_classic()+ggtitle("MSRP correlation with numeric variables")+labs(x = "Numeric Variables", y = 'Correlation')+theme(legend.position='none')
```
Plot numerical variables
```{r}
# msrp. log transform
p1 <- cars %>% ggplot(aes(x=msrp)) + geom_histogram(bins=20)
# logged: cars %>% ggplot(aes(x=log(msrp))) + geom_histogram(bins=20)

# msrp vs. year. 
p2 <- cars %>% ggplot(aes(x=year, y=msrp)) + geom_point()

# msrp vs. popularity
## Popularity is a weird variable. Score is set by brand rather than model. And Ford has the highest pop score by far, but certainly not the highest msrp, so I suspect this var won't be used for predicting msrp. can be set to categorical if necessary
p3 <- cars %>% ggplot(aes(x=popularity, y=msrp)) + geom_point()

# msrp vs. highway_mpg
p4 <- cars %>% ggplot(aes(x=highway_mpg, y=msrp)) + geom_point() + geom_smooth(method='lm')

# msrp vs. city_mpg
p5 <- cars %>% ggplot(aes(x=city_mpg, y=msrp)) + geom_point() + geom_smooth(method='lm')

# msrp vs. engine_hp
p6 <- cars %>% ggplot(aes(x=engine_hp, y=msrp)) + geom_point() + geom_smooth(method='lm')

# PLOT
plot_grid(p1, p2, p3, p4, p5, p6)
```

## Categorical variables

Correlation of categorical variables 
```{r warning=FALSE}
# caution: code takes a hot min to run
hetcor(cars$msrp, cars$engine_fuel_type, cars$model, cars$make, cars$engine_cylinders, cars$transmission_type, cars$driven_wheels, cars$number_of_doors, cars$market_category, cars$vehicle_size, cars$vehicle_style)

### it appears market_category has the highest correlation with msrp (0.7203), followed by engine_cylinders with 0.6. Of insignificant correlation are model (.012), driven wheels(0.055) and number of doors(.1179)
```
msrp vs. make, vehicle_style, and market_category (did not plot msrp vs. model bc unreadable at 65 levels)
```{r}
# msrp vs. make
p7 <- cars %>% ggplot(aes(x=make, y=msrp)) + geom_boxplot()

# msrp vs. vehicle_style
vs_levels <- cars %>% select(msrp, vehicle_style) %>%
  group_by(vehicle_style) %>% summarize(median(msrp)) %>% arrange(desc(`median(msrp)`))

cars$vehicle_style <- factor(cars$vehicle_style, ordered=TRUE, levels=vs_levels$vehicle_style)
# boxplot
p8 <- cars %>% ggplot(aes(y = msrp, fill = vehicle_style))+geom_boxplot()

# msrp vs market_category
cars %>% select(msrp, market_category) %>%
  ggplot(aes(x = msrp, fill=market_category))+
  geom_histogram()+theme(legend.position='none')
# boxplot
p9 <- cars %>% ggplot(aes(x=market_category, y=msrp)) + geom_boxplot()

# PLOT ALL
plot_grid(p7, p8, p9, nrow = 3)
```

```{r}
mc_levels <- cars %>% select(msrp, market_category) %>%
  group_by(market_category) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$market_category <- factor(cars$market_category, ordered = TRUE, levels = mc_levels$market_category)

summary(cars$market_category)
```

Plotting rest of the categorical variables together
```{r}
# msrp vs. engine_cylinders
# ordered levels
ec_levels <- cars %>% select(msrp, engine_cylinders) %>%
  group_by(engine_cylinders) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$engine_cylinders <- factor(cars$engine_cylinders, ordered=TRUE, levels=ec_levels$engine_cylinders)

p10 <- cars %>% ggplot(aes(x=engine_cylinders, y=msrp)) + geom_boxplot()

## msrp vs. engine_fuel_type
p11 <- cars %>% ggplot(aes(x=engine_fuel_type, y=msrp)) + geom_boxplot()

## msrp vs. driven_wheels
p12 <- cars %>% ggplot(aes(x=driven_wheels, y=msrp)) + geom_boxplot()

# msrp vs. transmission_type
tt_levels <- cars %>% select(msrp, transmission_type) %>%
  group_by(transmission_type) %>% summarize(mean(msrp)) %>% arrange((`mean(msrp)`))

cars$transmission_type <- factor(cars$transmission_type, ordered=TRUE, levels=tt_levels$transmission_type)

p13 <- cars %>% ggplot(aes(y = msrp, fill = transmission_type))+geom_boxplot()

# msrp vs. vehicle_size
# ordering vehicle_size
vsi_levels <- cars %>% select(msrp, vehicle_size) %>%
  group_by(vehicle_size) %>% summarize(median(msrp)) %>% arrange((`median(msrp)`))

cars$vehicle_size <- factor(cars$vehicle_size, ordered=TRUE, levels=vsi_levels$vehicle_size)

p14 <- cars %>% ggplot(aes(x=vehicle_size, y=msrp)) + geom_boxplot()

## msrp vs. number_of_doors
p15 <- cars %>% ggplot(aes(x=number_of_doors, y=msrp)) + geom_boxplot()

## PLOT ALL
plot_grid(p10, p11, p12, p13, p14, p15)
```

# Simple Model

Split data
```{r}
set.seed(7)
library(splitTools) # data partition

index <- partition(cars$logmsrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]
```
log(msrp). Correlation matrix with *numerical* variables : two diff plots. Which do yall prefer?
```{r}
cars <- cars %>% mutate(logmsrp = log(msrp))
cor.numvars <- cars %>% select(logmsrp, popularity, year, engine_hp, highway_mpg, city_mpg) %>% cor()

# correlogram / high correlation = blue color and larger size / library(corrplot) ; 
corrplot(cor.numvars, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

# correlation chart / library(GGally)
ggpairs(cars, columns = colnames(cor.numvars))

# popularity has low correlation but is required for the assignment. maybe run without it later
# include all num var in model. all corr > .2
# highway, city mpg have slightly higher corr with logmsrp when logged; keep unlogged bc negligible
```
log(msrp). Correlation with *categorical* variables : two diff plots
```{r warning=FALSE}
hetcor(cars$logmsrp, cars$engine_fuel_type, cars$engine_cylinders, cars$market_category, cars$vehicle_size, cars$make, cars$model, cars$transmission_type, cars$driven_wheels, cars$number_of_doors, cars$vehicle_style)
# caution: takes a few mins to run. here is output to save you time in desc order
# engine_cylinders : .5578 # keep in model
# engine_fuel_type : -.4082 # keep in model
# vehicle_size : .2766 # keep in model
# transmission_type : -.2544 # keep in model
# make : -.2271 # maybe keep in model
# market_category : -.1893
# driven_wheels : -.07438
# number_of_doors : -.06646
# vehicle_style : .04083
# model : -.01161

# eda in order of highest correlation with logmsrp; check if factors are ordered
# did not include make, model, market_category bc insanely high # of levels

pairs <- ggpairs(data=cars, columns = c('logmsrp', 'engine_cylinders', 'engine_fuel_type', 'vehicle_size', 'transmission_type', 'driven_wheels', 'number_of_doors', 'vehicle_style'), cardinality_threshold = 16)

plots <- lapply(1:pairs$ncol, function(j) getPlot(pairs, i = 1, j = j))
ggmatrix(
plots,
nrow = 1,
ncol = pairs$ncol,
xAxisLabels = pairs$xAxisLabels,
yAxisLabels = pairs$yAxisLabels
)
```
Custom variable selection
```{r}
# chose all numerical vars (corr > 20% w logmsrp) and top 4 cat. vars (corr > 25% w logmsrp)

mod1 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, data=cars)
ols_vif_tol(model) # high vif for engine_cylinders, transmission_type, city_mpg so remove them for mod2
mod2 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars) 
ols_vif_tol(mod2) # vif still high, remove engine_fuel_type for mod3
mod3 <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars)
ols_vif_tol(mod3) # vifs look great
ols_regress(mod3) # view model summary
# full data metrics: adjr2=.781 rmse=.211 ase=.044

# now to test/train data
set.seed(7)
library(splitTools) # data partition

index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]

simple.train <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=train)
# train metrics: adjr2=.779 rmse=212 ase=.045

ASE <- function(actual, predicted) {(mean ((actual - predicted)^2))}
ASE(test$logmsrp, predicted = predict(simple.train, test))
# test metrics: ase=0.04355828
```
Lasso with vars from mod3
```{r}
library(glmnet)
x=model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, train)
y=log(train$logmsrp)
xtest<-model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, test)
ytest<-log(test$logmsrp)

lasso.mod <- glmnet(x,y,alpha=1)
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out) # 11 predictors is optimal

bestlambda<-cv.out$lambda.min
bestlambda
lasso.pred <- predict(lasso.mod, s=bestlambda, newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO # 0.0003952727. 

coef(lasso.mod, s=bestlambda)
# predictors chosen: popularity, year, engine_hp, highway_mpg, engine_fuel_type, vehicle_style
```
Lasso with vars from mod1 (test ASE negligible difference compared to mod3 but more variables. keep previous model instead)
```{r}
set.seed(7)
index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]

x=model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, train)
y=log(train$logmsrp)
xtest<-model.matrix(logmsrp~popularity+year+engine_hp+highway_mpg+city_mpg+engine_cylinders+engine_fuel_type+vehicle_size+transmission_type, test)
ytest<-log(test$logmsrp)
lasso.mod <- glmnet(x,y,alpha=1)
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out) # 21 predictors chosen
bestlambda<-cv.out$lambda.min
bestlambda
lasso.pred <- predict(lasso.mod, s=bestlambda, newx=xtest)
testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO # 0.0003574083
coef(lasso.mod, s=bestlambda) # view predictors
```
Fit model, assumptions check
```{r}
simple <- lm(logmsrp~popularity+year+engine_hp+highway_mpg+engine_fuel_type+vehicle_size, data=cars)
ols_aic(simple) # aic = -2706.836
ols_plot_diagnostics(simple, print_plot = FALSE)
# residual histogram, cook's d chart looks great
# qq plot pretty good. some veering off at both extreme ends
# veryyyy few obs with outlier & leverage (expected for sample size this large)
# residual vs predicted looks good but some veering off at upper end
```
# Complex Model
```{r}

```

# Nonparametric Model
```{r}
# decision tree but each leaf represents a numberic value.
# splits are determined by trying a certain amount of thresholds and calculating Sum Squared Residuals
# for predictions, the average value of the bucket/leaf that an observation falls into is what is used as the predicted value. 
# a linear relationship between independent and dependent variables is no longer needed, and "interactions" do not need to be specified. 
# risk of overfitting by including too many variables that do not add value (bias/variance trade off) 
# risk of overfitting by including too many "leaves", too high of a tree depth
# this is mitigated by limiting minimum number of observations needed for a given bucket. Other methods can be done, such as ensuring fit improves CP metric by a certain amount, or limiting overall tree depth. 

#function for calculating ASE
ASE <- function(actual, predicted) {(mean ((actual - predicted)^2))}

set.seed(7)

library(splitTools) # data partition
index <- partition(cars$msrp, p = c(train = 0.8, valid = 0.1, test = 0.1))
train <- cars[index$train, ]
valid <- cars[index$valid, ]
test <- cars[index$test, ]

#first model using min bucket size of 100 to prevent overfitting
tree1 <- rpart(log(msrp) ~ highway_mpg + year + engine_hp + market_category + engine_cylinders, data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))
treep1 <- exp(predict(tree1, test))

rpart.plot(tree1, type = 1, digits = 3, fallen.leaves = TRUE)

ASE(test$msrp, treep1)

sqrt(ASE(test$msrp, treep1))

#second model using cross validation to determine max depth, optimizing for rmse
trctrl <- trainControl(method = 'repeatedcv', repeats = 3, number = 10)
tree2 <- train(log(msrp) ~ highway_mpg + year + engine_hp + market_category + engine_cylinders, trControl = trctrl, data = train, method = "rpart2")

treep2 <- exp(predict(tree2, test))

ASE(test$msrp, treep2)

sqrt(ASE(test$msrp, treep2))

#model three like first but using unlogged msrp
tree3 <- rpart(msrp ~ highway_mpg + year + engine_hp + market_category + engine_cylinders, data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))

treep3 <- predict(tree3, test)

rpart.plot(tree3, type = 1, digits = 3, fallen.leaves = TRUE)

ASE(test$msrp, treep3)

sqrt(ASE(test$msrp, treep3))

#model 4 with all variables and unlogged msrp
tree4 <- rpart(msrp ~., data = train, method = 'anova',
                    control = rpart.control(minbucket = 100))
treep4 <- predict(tree4, test)

rpart.plot(tree4, type = 1, digits = 3, fallen.leaves = TRUE)

ASE(test$msrp, treep4)

sqrt(ASE(test$msrp, treep4))

#graphing
test2 <- test
test2$predicted1 <- treep1
test2$predicted2 <- treep2
test2$predicted3 <- treep3
test2$predicted4 <- treep4
p1 <- test2 %>% ggplot(aes(x = predicted1, y = msrp)) + geom_point() + geom_smooth(method = 'lm')
p2 <- test2 %>% ggplot(aes(x = predicted2, y = msrp)) + geom_point() + geom_smooth(method = 'lm')
p3 <- test2 %>% ggplot(aes(x = predicted3, y = msrp)) + geom_point() + geom_smooth(method = 'lm')
p4 <- test2 %>% ggplot(aes(x = predicted4, y = msrp)) + geom_point() + geom_smooth(method = 'lm')

library(cowplot)
plot_grid(p1, p2, p3, p4)

# RMSE and AIC for each model. The logged models (1 + 2) do not compute properly with this function
library(regclass)

summarize_tree(tree1)

summarize_tree(tree2$finalModel)

summarize_tree(tree3)

summarize_tree(tree4)
```

